{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BAM:\n",
    "    \n",
    "    \n",
    "#     def __init__(n_inputs, n_outputs):\n",
    "#         self._\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: replicate basic experiments from knoblauch chapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weight arrays\n",
    "# later on make them sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_u = 10  # input neurons\n",
    "n_v = 10  # output neurons\n",
    "\n",
    "\n",
    "u = nd.zeros([n_u])\n",
    "v = nd.zeros([n_v])\n",
    "\n",
    "W = nd.zeros([n_v, n_u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_potential = W.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
       "<NDArray 10x10 @cpu(0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SynapseProperties:\n",
    "\n",
    "    p_g = 0.1\n",
    "\n",
    "    # probs named action-given-learning signal (e|s, c|s, d|s in the paper\n",
    "\n",
    "    p_elim_1 = 0\n",
    "    p_elim_0 = 0.01\n",
    "\n",
    "    p_cons_1 = 0.97\n",
    "    p_cons_0 = 0.03\n",
    "\n",
    "    p_deco_0 = 0.8\n",
    "    p_deco_1 = 0.2\n",
    "    \n",
    "    def __post_init__(self):\n",
    "\n",
    "        # for now make sure these two add to 1\n",
    "        assert np.isclose(1-self.p_cons_1, self.p_cons_0)\n",
    "        assert np.isclose(1-self.p_deco_0, self.p_deco_0)\n",
    "\n",
    "# @dataclass\n",
    "# class AssemblyProperties:\n",
    "    \n",
    "#     k = 3\n",
    "#     n = 10\n",
    "    \n",
    "#     # zip net rule parameter to set fraction of neuron's synapses that should be potentiated\n",
    "#     p_1 = 0\n",
    "    \n",
    "#     def __post_init(self):\n",
    "#         self.p_1 = self.k / self.n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance / homeostatic constants\n",
    "\n",
    "# ?? desired fraction of each neuron's synapses to be potentiated\n",
    "#  ... how the f do we maintain this? does this drive the inhibition somehow?\n",
    "# p_1 = 0.1 ???\n",
    "\n",
    "# some definitions from the paper \n",
    "\n",
    "# P       - anatomical connectivity: fraction of neuron pairs in an assembly connected by \n",
    "#           at least one actual synapse\n",
    "# P_pot   - fraction of neuron pairs connected by at least one potential synapse\n",
    "# P_eff   - fraction of \"required synapses\" (given a learning signal S_ij) that \n",
    "#           have been realized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key stuff\n",
    "# - cell assembly sizes must be limited to k for this to make sense, \n",
    "# thats a constraint on output patterns size if we want recurrent k-WTA inhibition to be \n",
    "# applied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire(W_synapse, u_input, k_wta):\n",
    "    \"\"\"Returns binary activation after k-wta is applied on the postsynaptic neurons\"\"\"\n",
    "    \n",
    "    v = W_synapse * u_input\n",
    "    \n",
    "    # zero out activations that are not in the top-k    \n",
    "    v = v.topk(k=k_wta, ret_type='mask')\n",
    "    return v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_homosynaptic(u, v, p):\n",
    "    \"\"\"Returns a matrix of pairs of values from u and v \n",
    "    using the homosynaptic rule with probability (mean activation) p\"\"\"\n",
    "    \n",
    "    #return -p * \n",
    "    \n",
    "    # TODO:\n",
    "    # may be able to use khatri_rao and some fanciness to make this \n",
    "    # vectorized on gpu later\n",
    "    \n",
    "    W = nd.zeros([u.shape[0], v.shape[0]])\n",
    "    for i, u_i in enumerate(u):\n",
    "        for j, v_j in enumerate(v):\n",
    "            W[i,j] = -p * u_i + (1*v_j)*u_i\n",
    "    \n",
    "    return W\n",
    "    #return -p * u + (1 * v)*u\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allclose(a, b):    \n",
    "    return nd.sum(nd.abs(a-b)) < 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert allclose(R_homosynaptic(nd.array([0, 1]), nd.array([0, 1]), p=.1), nd.array([[0,0],[-0.1, 0.9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.9]]\n",
       "<NDArray 1x1 @cpu(0)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_homosynaptic(nd.array([1]), nd.array([1]), p=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "class SimpleAssembly:\n",
    "    \n",
    "    properties: AssemblyProperties = AssemblyProperties()    \n",
    "    \n",
    "    def __init__(self, n_input, n_output, \n",
    "                 assembly_size, \n",
    "                 synapse_rule,\n",
    "                 potentiated_synapse_fraction,\n",
    "                \n",
    "                ):\n",
    "        self.n_u = n_input\n",
    "        self.n_v = n_output\n",
    "        \n",
    "        self.R = synapse_rule\n",
    "        self.k = assembly_size\n",
    "        self.p_1 = potentiated_synapse_fraction\n",
    "        self.u = nd.zeros([n_u])\n",
    "        self.v = nd.zeros([n_v])\n",
    "\n",
    "        self.W = nd.zeros([n_v, n_u])\n",
    "        self.R_totals = W.copy()\n",
    "\n",
    "    def fire(self, input_pattern):\n",
    "        \n",
    "        # check that its k or smaller in size?        \n",
    "        self.v = fire(W_synapse=self.W, u_input=input_pattern, k_wta=self.k)\n",
    "        return self.v\n",
    "\n",
    "    def update_weights(self):\n",
    "        \n",
    "        # use zip net rule for now\n",
    "        # TODO: figure out how the willshaw rule might apply in later stages of learning/\n",
    "        #  consolidation \n",
    "        \n",
    "        \n",
    "        self.R_totals += self.synapse_rule(self.u, self.v, p=self.k/ self.n_output)\n",
    "        \n",
    "        \n",
    "        # how to get just p1 synapses per (output) neuron potentiated?\n",
    "        # well each output neuron is a column in v\n",
    "        # number of active synapses is the number of non-zero values in the weight matrix\n",
    "        #  NEXT STEP: finish implementing this dynamic thresholding\n",
    "        #  -- do i really need to compute theta_ij dynamically for every neuron? like a second weight matrix?\n",
    "        #  -- or can i just take the topk at every timestep...hmmmmmm\n",
    "        \n",
    "        self.W = self.R_totals - self.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
